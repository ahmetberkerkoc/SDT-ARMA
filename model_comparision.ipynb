{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ahmet\\miniconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "import itertools\n",
    "\n",
    "#models\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "def MAPE(y_test, pred):\n",
    "    mape = np.mean(np.abs((y_test - pred) / y_test))\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = \"y\"\n",
    "test_size = 0.3\n",
    "date_column_name = \"date\"\n",
    "model_name = \"lightgbm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"DailyDelhiClimate.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path)\n",
    "df = df.drop(\"Unnamed: 0\",axis=1)\n",
    "\n",
    "if date_column_name is not None:\n",
    "    df = df.drop(date_column_name, axis=1)\n",
    "col_list = list(df.columns)\n",
    "col_list.remove(label_name)\n",
    "col_list.insert(0, label_name)\n",
    "\n",
    "df = df[col_list]\n",
    "\n",
    "y = df.loc[:,\"y\"]\n",
    "X = df.iloc[:,1:]\n",
    "\n",
    "data_len = len(X)\n",
    "forecast_horizion = int(data_len * test_size)\n",
    "\n",
    "X_train, X_test = X.iloc[:-forecast_horizion], X.iloc[-forecast_horizion:]\n",
    "y_train, y_test = y.iloc[:-forecast_horizion], y.iloc[-forecast_horizion:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dic = {\n",
    "    \"lightgbm\": lgb.LGBMRegressor(),\n",
    "    \"xgboost\": xgb.XGBRegressor(),\n",
    "    \"catboost\": cb.CatBoostRegressor(),\n",
    "    \"decision_tree\": DecisionTreeRegressor()\n",
    " }\n",
    "\n",
    "search_param_dic = {\n",
    "    \"lightgbm\": {\n",
    "            \"n_estimators\": [100, 250, 500, 750, 1000],\n",
    "            \"learning_rate\": [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "            \"num_leaves\": [15, 31, 63, 127, 255],\n",
    "            \"max_depth\": [4, 6, 7, 8, 10],\n",
    "            \"subsample\": [0.4, 0.6, 0.7, 0.9],\n",
    "            \"subsample_freq\": [1, 5, 10, 20, 50],\n",
    "            \"colsample_bytree\": [0.4, 0.6, 0.7, 0.9],\n",
    "            \"reg_alpha\": [0, 0.01, 0.05, 0.5, 1, 10],\n",
    "            \"reg_lambda\": [0, 0.01, 0.05, 0.5, 1, 10],\n",
    "            \"max_bin\": [15, 31, 63, 127, 255],\n",
    "            \"random_state\": [0],\n",
    "            \"verbose\": [-1],\n",
    "        },\n",
    "    \"xgboost\": {\n",
    "            \"n_estimators\": [100, 250, 500, 750, 1000],\n",
    "            \"learning_rate\": [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "            \"num_leaves\": [15, 31, 63, 127, 255],\n",
    "            \"max_depth\": [4, 6, 7, 8, 10],\n",
    "            \"subsample\": [0.4, 0.6, 0.7, 0.9],\n",
    "            \"subsample_freq\": [1, 5, 10, 20, 50],\n",
    "            \"colsample_bytree\": [0.4, 0.6, 0.7, 0.9],\n",
    "            \"reg_alpha\": [0, 0.01, 0.05, 0.5, 1, 10],\n",
    "            \"reg_lambda\": [0, 0.01, 0.05, 0.5, 1, 10],\n",
    "            \"max_bin\": [15, 31, 63, 127, 255],\n",
    "            \"random_state\": [0],\n",
    "            \"verbose\": [-1],\n",
    "        },\n",
    "    \"catboost\": {\n",
    "        \"iterations\": [100, 300, 500, 1000],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "        \"depth\": [4, 6, 8, 10],\n",
    "        \"l2_leaf_reg\": [1, 3, 5, 7, 9],\n",
    "        \"border_count\": [32, 64, 128, 254],\n",
    "        \"random_strength\": [0, 0.1, 0.5, 1],\n",
    "        \"random_seed\": [0],\n",
    "    }\n",
    ",\n",
    "    \"decision_tree\": {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [2, 3, 4, 5, 10, 12],\n",
    "        \"min_samples_split\": [2, 5, 10, 20],\n",
    "        \"min_samples_leaf\": [1, 2, 4, 10],\n",
    "}\n",
    " }\n",
    "\n",
    "\n",
    "model = model_dic[model_name]\n",
    "searching_params = search_param_dic[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold-out Validation\n",
    "xv_cls = RandomizedSearchCV\n",
    "val_size = forecast_horizion\n",
    "train_val_indexes = np.zeros_like(y_train)\n",
    "train_val_indexes[:-val_size] = -1\n",
    "fold_size = PredefinedSplit(test_fold=train_val_indexes)\n",
    "\n",
    "xv = xv_cls(estimator=model, param_distributions=searching_params, n_iter=10000, scoring=\"neg_mean_absolute_error\", n_jobs=-1,\n",
    "                    cv=fold_size, verbose=-1, refit=False)\n",
    "\n",
    "xv.fit(X_train, y_train)\n",
    "\n",
    "best_params = xv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X.iloc[:-forecast_horizion], X.iloc[-forecast_horizion:]\n",
    "y_train, y_test = y.iloc[:-forecast_horizion], y.iloc[-forecast_horizion:]\n",
    "\n",
    "best_model = type(model)(**best_params).fit(X_train, y_train)\n",
    "preds = best_model.predict(X_train)\n",
    "fores = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse_score = mse(y_train,preds)\n",
    "train_mae_score = mae(y_train,preds)\n",
    "train_mape_score = MAPE(y_train,preds)\n",
    "\n",
    "test_mse_score = mse(y_test,fores)\n",
    "test_mae_score = mae(y_test,fores)\n",
    "test_mape_score = MAPE(y_test,fores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_train,label = \"target\")\n",
    "plt.plot(preds,label=\"preds\")\n",
    "plt.legend()\n",
    "plt.title(\"Train Targets vs Train Preds\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(y_test,label = \"target\")\n",
    "plt.plot(fores,label=\"fores\")\n",
    "plt.legend()\n",
    "plt.title(\"Test Labels vs Test Forecasts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path)\n",
    "df = df.drop(\"Unnamed: 0\",axis=1)\n",
    "\n",
    "if date_column_name is not None:\n",
    "    df = df.drop(date_column_name, axis=1)\n",
    "col_list = list(df.columns)\n",
    "col_list.remove(label_name)\n",
    "col_list.insert(0, label_name)\n",
    "\n",
    "df = df[col_list]\n",
    "\n",
    "y = df.loc[:,\"y\"]\n",
    "X = df.iloc[:,1:]\n",
    "\n",
    "data_len = len(X)\n",
    "forecast_horizion = int(data_len * test_size)\n",
    "\n",
    "value = y\n",
    "train_value = X.iloc[:-forecast_horizion]\n",
    "test_value = X.iloc[-forecast_horizion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the parameter space for grid search\n",
    "p = d = q = range(0, 3)  # Example range, adjust as necessary\n",
    "P = D = Q = range(0, 3)  # Example range, adjust as necessary\n",
    "s = [12,24]  # Seasonal period, adjust based on your data's seasonality\n",
    "\n",
    "\n",
    "best_mse = float(\"inf\")\n",
    "for param in [(x[0], x[1], x[2]) for x in itertools.product(p, d, q)]:\n",
    "    for seasonal_param in [(x[0], x[1], x[2], x[3]) for x in itertools.product(P, D, Q, s)]:\n",
    "\n",
    "        arima_model = ARIMA(\n",
    "                        value[:-2*forecast_horizion],\n",
    "                        order=param,\n",
    "                        exog=train_value[:-forecast_horizion],\n",
    "                        seasonal_order=seasonal_param\n",
    "                    )\n",
    "        model = arima_model.fit()\n",
    "\n",
    "        start_index = len(train_value)-forecast_horizion\n",
    "        end_index = start_index + forecast_horizion - 1\n",
    "\n",
    "        forecast = model.predict(start=start_index, end=end_index, exog=train_value[-forecast_horizion:])\n",
    "\n",
    "        y_test_np = value[-2*forecast_horizion:-forecast_horizion].to_numpy()\n",
    "        forecast = forecast.to_numpy()\n",
    "        \n",
    "        mse_score = mse(y_test_np, forecast)\n",
    "        mae_score = mae(y_test_np, forecast)\n",
    "        mape_score = MAPE(y_test_np, forecast)\n",
    "        \n",
    "        if mse_score<best_mse:\n",
    "            best_mse = mse_score\n",
    "            best_order = param\n",
    "            best_seasonal_order = seasonal_param\n",
    "        \n",
    "        print(mse_score)\n",
    "        print(mae_score)\n",
    "        print(mape_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = ARIMA(\n",
    "                        value[:-forecast_horizion],\n",
    "                        order=best_order,\n",
    "                        exog=train_value,\n",
    "                        seasonal_order=best_seasonal_order\n",
    "                    )\n",
    "model = best_model.fit()\n",
    "\n",
    "start_index = len(train_value)\n",
    "end_index = start_index + forecast_horizion - 1\n",
    "\n",
    "forecast = model.predict(start=start_index, end=end_index, exog=test_value)\n",
    "\n",
    "y_test_np = value[-forecast_horizion:].to_numpy()\n",
    "forecast = forecast.to_numpy()\n",
    "\n",
    "mse_score = mse(y_test_np, forecast)\n",
    "mae_score = mae(y_test_np, forecast)\n",
    "mape_score = MAPE(y_test_np, forecast)\n",
    "\n",
    "print(mse_score)\n",
    "print(mae_score)\n",
    "print(mape_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.drop(\"Unnamed: 0\",axis=1)\n",
    "\n",
    "if date_column_name is not None:\n",
    "    df = df.drop(date_column_name, axis=1)\n",
    "col_list = list(df.columns)\n",
    "col_list.remove(label_name)\n",
    "col_list.insert(0, label_name)\n",
    "\n",
    "df = df[col_list]\n",
    "\n",
    "y = df.loc[:,\"y\"]\n",
    "X = df.iloc[:,1:]\n",
    "\n",
    "y = y.values\n",
    "X = X.values\n",
    "\n",
    "data_len = len(X)\n",
    "forecast_horizion = int(data_len * test_size)\n",
    "\n",
    "y_train_orig = y[:-forecast_horizion]\n",
    "y_test_orig = y[-forecast_horizion:]\n",
    "\n",
    "feature_shape = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize the data\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Create sequences for input and output\n",
    "def create_sequences(data, target, n_steps):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - n_steps):\n",
    "        X_seq.append(data[i : i + n_steps])\n",
    "        y_seq.append(target[i + n_steps])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "n_steps = 10  # number of time steps to look back\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, n_steps)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "X_train, X_test = X_seq[:-forecast_horizion], X_seq[-forecast_horizion:]\n",
    "y_train, y_test = y_seq[:-forecast_horizion], y_seq[-forecast_horizion:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, activation=\"relu\", input_shape=(n_steps, feature_shape)))\n",
    "model.add(Dense(units=1))\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_absolute_error\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"Training Loss: {train_loss}\")\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform the predictions to the original scale\n",
    "y_train_pred_inv = scaler_y.inverse_transform(y_train_pred).reshape(-1)\n",
    "y_test_pred_inv = scaler_y.inverse_transform(y_test_pred).reshape(-1)\n",
    "\n",
    "y_test = y_test_orig.copy()\n",
    "\n",
    "test_mse_score = mse(y_test, y_test_pred_inv)\n",
    "test_mae_score = mae(y_test, y_test_pred_inv)\n",
    "test_mape_score = MAPE(y_test, y_test_pred_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP - ARMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from SDT import SDT\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import random\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from SDT import MLPModel, MLPModel_deep, MLPModel_residual\n",
    "\n",
    "\n",
    "def mape(y_test, pred):\n",
    "    mape = np.mean(np.abs((y_test - pred) / y_test))\n",
    "    return mape\n",
    "\n",
    "\n",
    "exp_name = \"experiment\"\n",
    "\n",
    "\n",
    "model_type = \"mlp_model\" # mlp_model, deep_mlp_model, #mlp_model_residual_connections\n",
    "\n",
    "lr = 1e-2  # learning rate\n",
    "epochs = 30 # the number of training epochs\n",
    "\n",
    "if not os.path.exists(\"Results\"):\n",
    "    os.makedirs(\"Results\")\n",
    "\n",
    "result_folder = f\"Results/sdt_arm_{exp_name}\"\n",
    "result_txt_file = f\"sdt_arma_{exp_name}.txt\"\n",
    "\n",
    "if not os.path.exists(result_folder):\n",
    "    os.makedirs(result_folder)\n",
    "\n",
    "# Parameters\n",
    "# the number of input dimensions\n",
    "output_dim = 1  # the number of outputs (i.e., # classes on MNIST)\n",
    "batch_size = 1  # batch size\n",
    "use_cuda = torch.cuda.is_available()  # whether to use GPU\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Load data\n",
    "\n",
    "##########################\n",
    "\n",
    "mu, sigma = 0, 0.1\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "if date_column_name is not None:\n",
    "    df = df.drop(date_column_name, axis=1)\n",
    "col_list = list(df.columns)\n",
    "col_list.remove(label_name)\n",
    "col_list.insert(0, label_name)\n",
    "\n",
    "df = df[col_list]\n",
    "\n",
    "y = df.loc[:, label_name]\n",
    "X = df.iloc[:, 1:]\n",
    "data_len = len(X)\n",
    "\n",
    "forecast_horizon = int(data_len * test_size)\n",
    "e_t = np.random.normal(mu, sigma, len(X))\n",
    "\n",
    "X[\"e\"] = e_t\n",
    "X[\"e-1\"] = 0\n",
    "X[\"e-2\"] = 0\n",
    "X[\"e-3\"] = 0\n",
    "\n",
    "X_train, X_test = X.iloc[:-forecast_horizon].to_numpy().astype(np.float32), X.iloc[\n",
    "    -forecast_horizon:\n",
    "].to_numpy().astype(np.float32)\n",
    "y_train, y_test = y.iloc[:-forecast_horizon].to_numpy().astype(np.float32), y.iloc[\n",
    "    -forecast_horizon:\n",
    "].to_numpy().astype(np.float32)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_arr = scaler.fit_transform(X_train)\n",
    "X_test_arr = scaler.transform(X_test)\n",
    "\n",
    "y_train_arr = scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_arr = scaler.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "train_features = torch.Tensor(X_train_arr).to(device)\n",
    "train_targets = torch.Tensor(y_train_arr).to(device)\n",
    "test_features = torch.Tensor(X_test_arr).to(device)\n",
    "test_targets = torch.Tensor(y_test_arr).to(device)\n",
    "\n",
    "train = TensorDataset(train_features, train_targets)\n",
    "test = TensorDataset(test_features, test_targets)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "input_dim = train_features.shape[1]\n",
    "print(f\"input_dim: {input_dim}\")\n",
    "##########################\n",
    "\n",
    "# Model and Optimizer\n",
    "if model_name == \"mlp_model\":\n",
    "    model = MLPModel(input_dim) \n",
    "elif model_name == \"deep_mlp_model\":\n",
    "    model = MLPModel_deep(input_dim)\n",
    "elif model_name == \"mlp_model_residual_connections\":\n",
    "    model = MLPModel_residual(input_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"###############\")\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_target_list = []\n",
    "    train_output_list = []\n",
    "    e_1 = 0\n",
    "    e_2 = 0\n",
    "    e_3 = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data[0][-3] = e_1\n",
    "        data[0][-2] = e_2\n",
    "        data[0][-1] = e_3\n",
    "\n",
    "        batch_size = data.size()[0]\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # target_onehot = onehot_coding(target, device, output_dim)\n",
    "\n",
    "        output = model.forward(data)\n",
    "\n",
    "        e_3 = e_2\n",
    "        e_2 = e_1\n",
    "        e_1 = target.item() - output.item()\n",
    "\n",
    "        train_output_list.append(output.cpu().detach().numpy())\n",
    "        train_target_list.append(target.cpu().detach().numpy())\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_target_list = np.array(train_target_list).ravel()\n",
    "    train_output_list = np.array(train_output_list).ravel()\n",
    "\n",
    "    train_target_list = scaler.inverse_transform(train_target_list.reshape(-1, 1))\n",
    "    train_output_list = scaler.inverse_transform(train_output_list.reshape(-1, 1))\n",
    "\n",
    "    train_mse = mse(train_target_list, train_output_list)\n",
    "    train_mae = mae(train_target_list, train_output_list)\n",
    "    train_mape = mape(train_target_list, train_output_list)\n",
    "    print(f\"traim mse {train_mse}\")\n",
    "    print(f\"train mae {train_mae}\")\n",
    "    print(f\"train mape {train_mape}\")\n",
    "    print(\"--------\")\n",
    "\n",
    "    train_losses.append((train_mse, train_mae, train_mape))\n",
    "\n",
    "    # Evaluating\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    output_list = []\n",
    "    target_list = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data[0][-3] = e_1\n",
    "        data[0][-2] = e_2\n",
    "        data[0][-1] = e_3\n",
    "\n",
    "        batch_size = data.size()[0]\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model.forward(data)\n",
    "\n",
    "        e_3 = e_2\n",
    "        e_2 = e_1\n",
    "        e_1 = target.item() - output.item()\n",
    "\n",
    "        output_list.append(output.cpu().detach().numpy())\n",
    "        target_list.append(target.cpu().detach().numpy())\n",
    "\n",
    "    target_list = np.array(target_list).ravel()\n",
    "    output_list = np.array(output_list).ravel()\n",
    "\n",
    "    target_list = scaler.inverse_transform(target_list.reshape(-1, 1))\n",
    "    output_list = scaler.inverse_transform(output_list.reshape(-1, 1))\n",
    "\n",
    "    test_mse = mse(target_list, output_list)\n",
    "    test_mae = mae(target_list, output_list)\n",
    "    test_mape = mape(target_list, output_list)\n",
    "    print(f\"test mse {test_mse}\")\n",
    "    print(f\"test mae {test_mae}\")\n",
    "    print(f\"test mape {test_mape}\")\n",
    "\n",
    "    test_losses.append((test_mse, test_mae, test_mape))\n",
    "\n",
    "    print(\"###############\")\n",
    "\n",
    "error = (target_list - output_list) ** 2\n",
    "cum = np.cumsum(error) / (1 + np.arange(len(error)))\n",
    "\n",
    "# print(str(cum))\n",
    "\n",
    "plt.plot(target_list)\n",
    "plt.plot(output_list)\n",
    "plt.savefig(f\"{result_folder}/predictions_{exp_name}.png\")\n",
    "plt.cla()\n",
    "f = open(f\"{result_folder}/{result_txt_file}\", \"a+\")\n",
    "f.write(\"########\\n\")\n",
    "f.write(f\"Data Type: {exp_name}\\n\")\n",
    "\n",
    "f.write(\"Train Losses\")\n",
    "f.write(str(train_losses) + \"\\n\\n\")\n",
    "\n",
    "f.write(\"Test Losses\")\n",
    "f.write(str(test_losses) + \"\\n\\n\")\n",
    "\n",
    "f.write(\"Cum Losses\" + \"\\n\\n\")\n",
    "f.write((str(cum)))\n",
    "\n",
    "f.write(\"########\\n\")\n",
    "f.close()\n",
    "\n",
    "test_losses = [l[0] for l in test_losses]\n",
    "train_losses = [l[0] for l in train_losses]\n",
    "\n",
    "plt.plot(test_losses, label=\"test\")\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE Losses\")\n",
    "plt.savefig(f\"{result_folder}/MLP_result_{exp_name}.png\")\n",
    "plt.cla()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from SDT import SDT\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import random\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from SDT import MLPModel, MLPModel_deep, MLPModel_residual\n",
    "\n",
    "\n",
    "def mape(y_test, pred):\n",
    "    mape = np.mean(np.abs((y_test - pred) / y_test))\n",
    "    return mape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "exp_name = \"experiment\"\n",
    "\n",
    "\n",
    "\n",
    "model_type = \"mlp_model\" # mlp_model, deep_mlp_model, #mlp_model_residual_connections\n",
    "\n",
    "lr = 1e-2  # learning rate\n",
    "epochs = 30 # the number of training epochs\n",
    "\n",
    "if not os.path.exists(\"Results\"):\n",
    "    os.makedirs(\"Results\")\n",
    "\n",
    "result_folder = f\"Results/sdt_arm_{exp_name}\"\n",
    "result_txt_file = f\"sdt_arma_{exp_name}.txt\"\n",
    "\n",
    "if not os.path.exists(result_folder):\n",
    "    os.makedirs(result_folder)\n",
    "\n",
    "# Parameters\n",
    "# the number of input dimensions\n",
    "output_dim = 1  # the number of outputs (i.e., # classes on MNIST)\n",
    "batch_size = 1  # batch size\n",
    "use_cuda = torch.cuda.is_available()  # whether to use GPU\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Load data\n",
    "\n",
    "##########################\n",
    "\n",
    "mu, sigma = 0, 0.1\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "if date_column_name is not None:\n",
    "    df = df.drop(date_column_name, axis=1)\n",
    "col_list = list(df.columns)\n",
    "col_list.remove(label_name)\n",
    "col_list.insert(0, label_name)\n",
    "\n",
    "df = df[col_list]\n",
    "\n",
    "y = df.loc[:, label_name]\n",
    "X = df.iloc[:, 1:]\n",
    "data_len = len(X)\n",
    "\n",
    "forecast_horizon = int(data_len * test_size)\n",
    "e_t = np.random.normal(mu, sigma, len(X))\n",
    "\n",
    "X[\"e\"] = e_t\n",
    "X[\"e-1\"] = 0\n",
    "X[\"e-2\"] = 0\n",
    "X[\"e-3\"] = 0\n",
    "\n",
    "X_train, X_test = X.iloc[:-forecast_horizon].to_numpy().astype(np.float32), X.iloc[\n",
    "    -forecast_horizon:\n",
    "].to_numpy().astype(np.float32)\n",
    "y_train, y_test = y.iloc[:-forecast_horizon].to_numpy().astype(np.float32), y.iloc[\n",
    "    -forecast_horizon:\n",
    "].to_numpy().astype(np.float32)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_arr = scaler.fit_transform(X_train)\n",
    "X_test_arr = scaler.transform(X_test)\n",
    "\n",
    "y_train_arr = scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_arr = scaler.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "train_features = torch.Tensor(X_train_arr).to(device)\n",
    "train_targets = torch.Tensor(y_train_arr).to(device)\n",
    "test_features = torch.Tensor(X_test_arr).to(device)\n",
    "test_targets = torch.Tensor(y_test_arr).to(device)\n",
    "\n",
    "train = TensorDataset(train_features, train_targets)\n",
    "test = TensorDataset(test_features, test_targets)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "input_dim = train_features.shape[1]\n",
    "print(f\"input_dim: {input_dim}\")\n",
    "##########################\n",
    "\n",
    "# Model and Optimizer\n",
    "\n",
    "if model_name == \"mlp_model\":\n",
    "    model = MLPModel(input_dim) \n",
    "elif model_name == \"deep_mlp_model\":\n",
    "    model = MLPModel_deep(input_dim)\n",
    "elif model_name == \"mlp_model_residual_connections\":\n",
    "    model = MLPModel_residual(input_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"###############\")\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_target_list = []\n",
    "    train_output_list = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        batch_size = data.size()[0]\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # target_onehot = onehot_coding(target, device, output_dim)\n",
    "\n",
    "        output = model.forward(data)\n",
    "\n",
    "        train_output_list.append(output.cpu().detach().numpy())\n",
    "        train_target_list.append(target.cpu().detach().numpy())\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_target_list = np.array(train_target_list).ravel()\n",
    "    train_output_list = np.array(train_output_list).ravel()\n",
    "\n",
    "    train_target_list = scaler.inverse_transform(train_target_list.reshape(-1, 1))\n",
    "    train_output_list = scaler.inverse_transform(train_output_list.reshape(-1, 1))\n",
    "\n",
    "    train_mse = mse(train_target_list, train_output_list)\n",
    "    train_mae = mae(train_target_list, train_output_list)\n",
    "    train_mape = mape(train_target_list, train_output_list)\n",
    "    print(f\"traim mse {train_mse}\")\n",
    "    print(f\"train mae {train_mae}\")\n",
    "    print(f\"train mape {train_mape}\")\n",
    "    print(\"--------\")\n",
    "\n",
    "    train_losses.append((train_mse, train_mae, train_mape))\n",
    "\n",
    "    # Print training status\n",
    "    # if batch_idx % log_interval == 0:\n",
    "    #     pred = output.data.max(1)[1]\n",
    "    #     correct = pred.eq(target.view(-1).data).sum()\n",
    "\n",
    "    # msg = \"Epoch: {:02d} | Batch: {:03d} | Loss: {:.5f} |\" \" Correct: {:03d}/{:03d}\"\n",
    "    # print(msg.format(epoch, batch_idx, loss, correct, batch_size))\n",
    "    # training_loss_list.append(loss.cpu().data.numpy())\n",
    "\n",
    "    # Evaluating\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    output_list = []\n",
    "    target_list = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        batch_size = data.size()[0]\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model.forward(data)\n",
    "\n",
    "        output_list.append(output.cpu().detach().numpy())\n",
    "        target_list.append(target.cpu().detach().numpy())\n",
    "\n",
    "    target_list = np.array(target_list).ravel()\n",
    "    output_list = np.array(output_list).ravel()\n",
    "\n",
    "    target_list = scaler.inverse_transform(target_list.reshape(-1, 1))\n",
    "    output_list = scaler.inverse_transform(output_list.reshape(-1, 1))\n",
    "\n",
    "    test_mse = mse(target_list, output_list)\n",
    "    test_mae = mae(target_list, output_list)\n",
    "    test_mape = mape(target_list, output_list)\n",
    "    print(f\"test mse {test_mse}\")\n",
    "    print(f\"test mae {test_mae}\")\n",
    "    print(f\"test mape {test_mape}\")\n",
    "\n",
    "    test_losses.append((test_mse, test_mae, test_mape))\n",
    "\n",
    "    print(\"###############\")\n",
    "\n",
    "error = (target_list - output_list) ** 2\n",
    "cum = np.cumsum(error) / (1 + np.arange(len(error)))\n",
    "\n",
    "error_train = (target_list - output_list) ** 2\n",
    "cum_train = np.cumsum(error_train) / (1 + np.arange(len(error_train)))\n",
    "\n",
    "# print(str(cum_train))\n",
    "\n",
    "plt.plot(target_list)\n",
    "plt.plot(output_list)\n",
    "plt.savefig(f\"{result_folder}/predictions_{exp_name}.png\")\n",
    "plt.cla()\n",
    "f = open(f\"{result_folder}/{result_txt_file}\", \"a+\")\n",
    "f.write(\"########\\n\")\n",
    "f.write(f\"M4 index: {exp_name}\\n\")\n",
    "\n",
    "f.write(\"Train Losses\")\n",
    "f.write(str(train_losses) + \"\\n\\n\")\n",
    "\n",
    "f.write(\"Test Losses\")\n",
    "f.write(str(test_losses) + \"\\n\\n\")\n",
    "\n",
    "f.write(\"Cum Losses\" + \"\\n\\n\")\n",
    "f.write((str(cum)))\n",
    "\n",
    "f.write(\"########\\n\")\n",
    "f.close()\n",
    "\n",
    "test_losses = [l[0] for l in test_losses]\n",
    "train_losses = [l[0] for l in train_losses]\n",
    "\n",
    "plt.plot(test_losses, label=\"test\")\n",
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE Losses\")\n",
    "plt.savefig(f\"{result_folder}/MLP_result_{exp_name}.png\")\n",
    "plt.cla()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOFT AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from SDT import SDT\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import random\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "\n",
    "def mape(y_test, pred):\n",
    "    mape = np.mean(np.abs((y_test - pred) / y_test))\n",
    "    return mape\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "\n",
    "    exp_name = \"soft_ar_experiment\"\n",
    "    data_path = data_path\n",
    "    label_name = label_name\n",
    "    test_size = test_size\n",
    "    date_column_name = date_column_name\n",
    "\n",
    "    depth = 3  # tree depth\n",
    "    lamda = 1e-3  # coefficient of the regularization term\n",
    "    lr = 1e-2   # learning rate\n",
    "    epochs = 30  # the number of training epochs\n",
    "\n",
    "    if not os.path.exists(\"Results\"):\n",
    "        os.makedirs(\"Results\")\n",
    "\n",
    "    result_folder = f\"Results/sdt_arm_{exp_name}\"\n",
    "    result_txt_file = f\"sdt_arma_{exp_name}.txt\"\n",
    "\n",
    "    if not os.path.exists(result_folder):\n",
    "        os.makedirs(result_folder)\n",
    "\n",
    "    # Parameters\n",
    "    # the number of input dimensions\n",
    "    output_dim = 1  # the number of outputs (i.e., # classes on MNIST)\n",
    "    batch_size = 1  # batch size\n",
    "    use_cuda = torch.cuda.is_available()  # whether to use GPU\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # reproducibility\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    random.seed(0)\n",
    "\n",
    "    # Load data\n",
    "\n",
    "    ##########################\n",
    "\n",
    "    mu, sigma = 0, 0.1\n",
    "\n",
    "    df = pd.read_csv(data_path)\n",
    "    df = df.drop(\"Unnamed: 0\", axis=1)\n",
    "    if date_column_name is not None:\n",
    "        df = df.drop(date_column_name, axis=1)\n",
    "    col_list = list(df.columns)\n",
    "    col_list.remove(label_name)\n",
    "    col_list.insert(0, label_name)\n",
    "\n",
    "    df = df[col_list]\n",
    "\n",
    "    y = df.loc[:, label_name]\n",
    "    X = df.iloc[:, 1:]\n",
    "    data_len = len(X)\n",
    "\n",
    "    forecast_horizon = int(data_len * test_size)\n",
    "    e_t = np.random.normal(mu, sigma, len(X))\n",
    "\n",
    "    X[\"e\"] = e_t\n",
    "    X[\"e-1\"] = 0\n",
    "    X[\"e-2\"] = 0\n",
    "    X[\"e-3\"] = 0\n",
    "\n",
    "    X_train, X_test = X.iloc[:-forecast_horizon].to_numpy().astype(np.float32), X.iloc[\n",
    "        -forecast_horizon:\n",
    "    ].to_numpy().astype(np.float32)\n",
    "    y_train, y_test = y.iloc[:-forecast_horizon].to_numpy().astype(np.float32), y.iloc[\n",
    "        -forecast_horizon:\n",
    "    ].to_numpy().astype(np.float32)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_arr = scaler.fit_transform(X_train)\n",
    "    X_test_arr = scaler.transform(X_test)\n",
    "\n",
    "    y_train_arr = scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "    y_test_arr = scaler.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    train_features = torch.Tensor(X_train_arr).to(device)\n",
    "    train_targets = torch.Tensor(y_train_arr).to(device)\n",
    "    test_features = torch.Tensor(X_test_arr).to(device)\n",
    "    test_targets = torch.Tensor(y_test_arr).to(device)\n",
    "\n",
    "    train = TensorDataset(train_features, train_targets)\n",
    "    test = TensorDataset(test_features, test_targets)\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_dim = train_features.shape[1]\n",
    "    print(f\"input_dim: {input_dim}\")\n",
    "    ##########################\n",
    "\n",
    "    # Model and Optimizer\n",
    "    tree = SDT(input_dim, output_dim, depth, lamda, use_cuda)\n",
    "    tree = tree.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(tree.parameters(), lr=lr)\n",
    "\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        time_start = time.time()\n",
    "        print(\"###############\")\n",
    "        print(f\"epoch: {epoch}\")\n",
    "        # Training\n",
    "        tree.train()\n",
    "        train_target_list = []\n",
    "        train_output_list = []\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            batch_size = data.size()[0]\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # target_onehot = onehot_coding(target, device, output_dim)\n",
    "\n",
    "            output, penalty = tree.forward(data, is_training_data=True)\n",
    "\n",
    "            train_output_list.append(output.cpu().detach().numpy())\n",
    "            train_target_list.append(target.cpu().detach().numpy())\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            loss += penalty\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_target_list = np.array(train_target_list).ravel()\n",
    "        train_output_list = np.array(train_output_list).ravel()\n",
    "        time_end = time.time()\n",
    "        print(f\"Train Time {time_end-time_start}\")\n",
    "        train_target_list = scaler.inverse_transform(train_target_list.reshape(-1, 1))\n",
    "        train_output_list = scaler.inverse_transform(train_output_list.reshape(-1, 1))\n",
    "\n",
    "        train_mse = mse(train_target_list, train_output_list)\n",
    "        train_mae = mae(train_target_list, train_output_list)\n",
    "        train_mape = mape(train_target_list, train_output_list)\n",
    "        print(f\"traim mse {train_mse}\")\n",
    "        print(f\"train mae {train_mae}\")\n",
    "        print(f\"train mape {train_mape}\")\n",
    "        print(\"--------\")\n",
    "\n",
    "        train_losses.append((train_mse, train_mae, train_mape))\n",
    "    \n",
    "\n",
    "        # Evaluating\n",
    "        tree.eval()\n",
    "        correct = 0.0\n",
    "        output_list = []\n",
    "        target_list = []\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            time_start_test = time.time()\n",
    "            batch_size = data.size()[0]\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = tree.forward(data)\n",
    "\n",
    "            output_list.append(output.cpu().detach().numpy())\n",
    "            target_list.append(target.cpu().detach().numpy())\n",
    "            time_end_test  =time.time()\n",
    "        target_list = np.array(target_list).ravel()\n",
    "        output_list = np.array(output_list).ravel()\n",
    "\n",
    "        target_list = scaler.inverse_transform(target_list.reshape(-1, 1))\n",
    "        output_list = scaler.inverse_transform(output_list.reshape(-1, 1))\n",
    "\n",
    "        test_mse = mse(target_list, output_list)\n",
    "        test_mae = mae(target_list, output_list)\n",
    "        test_mape = mape(target_list, output_list)\n",
    "        print(f\"test mse {test_mse}\")\n",
    "        print(f\"test mae {test_mae}\")\n",
    "        print(f\"test mape {test_mape}\")\n",
    "\n",
    "        test_losses.append((test_mse, test_mae, test_mape))\n",
    "\n",
    "        print(\"###############\")\n",
    "\n",
    "    error = (target_list - output_list) ** 2\n",
    "    cum = np.cumsum(error) / (1 + np.arange(len(error)))\n",
    "\n",
    "    plt.plot(target_list)\n",
    "    plt.plot(output_list)\n",
    "    plt.savefig(f\"{result_folder}/predictions_{exp_name}.png\")\n",
    "    plt.cla()\n",
    "    f = open(f\"{result_folder}/{result_txt_file}\", \"a+\")\n",
    "    f.write(\"########\\n\")\n",
    "    f.write(f\"M4 index: {exp_name}\\n\")\n",
    "\n",
    "    f.write(\"Train Losses\")\n",
    "    f.write(str(train_losses) + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"Test Losses\")\n",
    "    f.write(str(test_losses) + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"Cum Losses\" + \"\\n\\n\")\n",
    "    f.write((str(cum)))\n",
    "\n",
    "    f.write(\"########\\n\")\n",
    "    f.close()\n",
    "\n",
    "    test_losses = [l[0] for l in test_losses]\n",
    "    train_losses = [l[0] for l in train_losses]\n",
    "\n",
    "    plt.plot(test_losses, label=\"test\")\n",
    "    plt.plot(train_losses, label=\"train\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE Losses\")\n",
    "    plt.savefig(f\"{result_folder}/SDT_result_{exp_name}.png\")\n",
    "    plt.cla()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
